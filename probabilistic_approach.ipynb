{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "F3lWRhzSTFO9"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import difflib                        # A library that computes the difference between two strings, unused (was for further exploration and info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "cCxi6d94TFPR"
      },
      "outputs": [],
      "source": [
        "def look_changes(morphs, probs, threshold):\n",
        "  \"\"\"\n",
        "  A very naïve approach that searches for potential candidates for the changes.\n",
        "  It simply regroupes the different changes that have been recorded to have had one amongst morphological attributes\n",
        "  :params:  * morphs: (array) morphological attributes, mainly on the test data set\n",
        "            * Probs: A dictionary that regroupe changes to their morphological attributes and there occurences on training dataset\n",
        "            * Threshold: Unused, but serves as an indication to ignore underrated examples\n",
        "  :returns: A dictionnary with potential changes to make to a lemma, and the corresponding accumulative probabilities\n",
        "  within the training dataset\n",
        "  \"\"\"\n",
        "  possible_predictions = dict()\n",
        "  for c in probs.keys():\n",
        "    dict_probs = probs[c]\n",
        "    for morph in morphs:\n",
        "      if morph in dict_probs.keys() and dict_probs[morph] >= threshold:\n",
        "        if c in possible_predictions.keys():\n",
        "          possible_predictions[c] += dict_probs[morph]\n",
        "        else:\n",
        "          possible_predictions[c] = dict_probs[morph]\n",
        "  return possible_predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "SLU9fXZsTFPU"
      },
      "outputs": [],
      "source": [
        "def stringDif(s, c):\n",
        "  \"\"\"\n",
        "  Looks up the prefix and suffix of a lemma c. The maximal substring in s that correspond to lemma is extracted\n",
        "  :params: s for the form and c for the lemma\n",
        "  :return: the prefix+\"-\"+suffix, where \"-\" replaces the inducted substring of the lemma c\n",
        "  \"\"\"\n",
        "  # Perte d'information, quoi faire pour rattaraper ?\n",
        "  for i in range(len(c),0, -1):\n",
        "    np = c[:i]\n",
        "    ns = s.replace(np, '-')\n",
        "    if ns != s:\n",
        "      # The substring found, and is replaced\n",
        "      break\n",
        "  return ns"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def look_obvious_changes(morphs, probs):\n",
        "  \"\"\"\n",
        "  An approach that searches for potential candidates for the changes.\n",
        "  It simply regroupes the different changes that have been recorded to have had *the exact same* morphological attributes\n",
        "  :params:  * morphs: (String) morphological attributes, mainly on the test data set\n",
        "            * Probs: A dictionary that regroupe changes to their morphological attributes and there occurences on training dataset\n",
        "  :returns: A dictionnary with potential changes to make to a lemma, and the corresponding occurence within the training dataset\n",
        "  \"\"\"\n",
        "  possible_predictions = dict()\n",
        "  for c in probs.keys():\n",
        "    dict_probs = probs[c]\n",
        "    if set(morphs.split(\";\")) == set(dict_probs.keys()):\n",
        "      if c in possible_predictions.keys():\n",
        "        possible_predictions[c] += 1\n",
        "      else:\n",
        "        possible_predictions[c] = 1\n",
        "    # If an example is not met on the training set, we abandon the cause. Hence, obvious\n",
        "  return possible_predictions\n",
        "\n",
        "def look_less_obvious_changes(morphs, probs):\n",
        "  \"\"\"Same as above, but records the cases where only one of the morphological attributes is present\"\"\"\n",
        "  possible_predictions = dict()\n",
        "  for c in probs.keys():\n",
        "    dict_probs = probs[c]\n",
        "    if np.all(list(map(lambda a : a in dict_probs.keys(), morphs.split(\";\")))):\n",
        "      # This is the case where the training set has a sample that had the exacte Morph. Att. and is amplified (+2)\n",
        "      if c in possible_predictions.keys():\n",
        "        possible_predictions[c] += 2\n",
        "      else:\n",
        "        possible_predictions[c] = 2\n",
        "    # The case where at least one morph. att. is present. This case is less representable (+1) \n",
        "    elif np.any(list(map(lambda a : a in dict_probs.keys(), morphs.split(\";\")))):\n",
        "      if c in possible_predictions.keys():\n",
        "        possible_predictions[c] += 1\n",
        "      else:\n",
        "        possible_predictions[c] = 1\n",
        "    # +0 for the case of no presence. This occurences and amplification serve for the probabilistic model later on.\n",
        "  return possible_predictions\n",
        "\n",
        "def select_less_obvious(lemma, morphs, corresp, corresp2, corresp3, dist):\n",
        "  \"\"\"\n",
        "    This is an advanced method. It takes to account some learned information from the training set to \n",
        "    select a potential candidates. It, all the same, is still a naïve approach.\n",
        "    :params:  * corresp: Contains, as probs before, the changes \"prefix-suffix\", the attributes and occurences that correspond to them.\n",
        "              * corresp2: Contains changes and the corresponding lemmas.\n",
        "              * corresp3: Contains the changes and their occurences within the training data set (the world)\n",
        "              * dist: Distance to defines as one pleases between two lemmas\n",
        "    :return: The most probable changes taking to account:\n",
        "    * Their M.A. and those of the test set\n",
        "    * The probability that these changes occur (if they accur rarely in the training set, so they should in the test set)\n",
        "    * The distance of the new lemma from the lemmas in the training set (if this lemma is so close to another one, so the changes are \n",
        "    most probably the same as those on the training set)\n",
        "  \"\"\"\n",
        "  pp = look_less_obvious_changes(morphs, fromCorrespToProbs(corresp))\n",
        "  new_preds = dict.fromkeys(pp.keys())\n",
        "  for k in pp.keys():\n",
        "    dists = list(map(lambda a: dist(a, lemma),corresp3[k]))\n",
        "    new_preds[k] = pp[k] * corresp2[k] * (np.max(dists) - np.mean(dists))\n",
        "  minIdx = np.argmax(list(new_preds.values()))\n",
        "  return np.array(list(new_preds.keys()))[minIdx]"
      ],
      "metadata": {
        "id": "etJI5CrspyuV"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def createTrainDatasetFromFD(fd):\n",
        "  \"\"\"\n",
        "    Simple function creating the tuple (lemma, form, M.A) also the tuple (corresp, corresp2, corresp3) as:\n",
        "              * corresp: The changes \"prefix-suffix\", the attributes and occurences that correspond to them.\n",
        "              * corresp2: Contains changes and the corresponding lemmas.\n",
        "              * corresp3: Contains the changes and their occurences within the training data set (the world)\n",
        "    :params: fd is a file descriptor\n",
        "  \"\"\"\n",
        "  # Create words\n",
        "  l = fd.readline()\n",
        "  ws, fs, rs = list(), list(), list()\n",
        "  while l:\n",
        "      w, f, r = list( map(lambda a : a.strip(), l.split('\\t')) )\n",
        "      ws.append(w); fs.append(f); rs.append(r)\n",
        "      l = fd.readline()\n",
        "  # Create a dictionnary with rule (pre-suf) as keys and have the corresponding morph attributes \n",
        "  corresp = dict()\n",
        "  corresp2 = dict()\n",
        "  corresp3 = dict()\n",
        "  for i in range(len(ws)):\n",
        "    rule = stringDif(fs[i], ws[i])\n",
        "    if rule in corresp.keys():\n",
        "      corresp2[rule] += 1\n",
        "      corresp3[rule].append(ws[i])\n",
        "      for k in rs[i].split(\";\"):\n",
        "        if k in corresp[rule]:\n",
        "          corresp[rule][k] += 1\n",
        "        else:\n",
        "          corresp[rule][k] = 1\n",
        "    else:\n",
        "        corresp[rule] =  {key: 1 for key in rs[i].split(\";\")}\n",
        "        corresp2[rule] = 1\n",
        "        corresp3[rule] = [ws[i]]\n",
        "  return (ws, fs, rs), (corresp, corresp2, corresp3)\n",
        "\n",
        "def testDatasetFromFD(fd):\n",
        "  \"\"\"\n",
        "    In the case of test datasets, the tuple (lemma, M.A) only is returned\n",
        "  \"\"\"\n",
        "  l = fd.readline()\n",
        "  ws, rs = list(), list()\n",
        "  while l:\n",
        "      w, r = list( map(lambda a : a.strip(), l.split('\\t')) )\n",
        "      ws.append(w); rs.append(r)\n",
        "      l = fd.readline()\n",
        "  return (ws, rs)\n",
        "\n",
        "def fromCorrespToProbs(corresp):\n",
        "  \"\"\"\n",
        "    A function that changes occurences in corresp to probabilities\n",
        "    :Example:\n",
        "    > corresp = {\"pre-é\": {\"V\":7, \"Noun\":3}, \"dé-é\" : {\"V\":8, \"Adj\":2}}\n",
        "    > probs   = {\"pre-é\": {\"V\":0.7, \"Noun\":0.3}, \"dé-é\" : {\"V\":0.8, \"Adj\":0.2}}\n",
        "  \"\"\"\n",
        "  probs = corresp.copy()\n",
        "  for key1 in probs.keys():\n",
        "    sumValues = sum(probs[key1].values())\n",
        "    for key2 in probs[key1].keys():\n",
        "      probs[key1][key2] = probs[key1][key2] / sumValues\n",
        "  return probs"
      ],
      "metadata": {
        "id": "cjWeiBHS_N6R"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def selectMaxFromDict(dictio):\n",
        "  \"\"\"\n",
        "    Selects the key that correspond to the highest value in a dictionary\n",
        "  \"\"\"\n",
        "  ks = np.array(list(dictio.keys()))\n",
        "  vs = np.array(list(dictio.values()))\n",
        "  ord = np.argsort(vs)\n",
        "  ks_n = ks[ord]\n",
        "  return ks_n[0] if not ks_n.shape[0] == 0 else '-'\n",
        "\n",
        "def selectFromDict(dictio, threshold):\n",
        "  \"\"\"\n",
        "    Selects cases (keys) from dictionary where the values are >= threshold\n",
        "  \"\"\"\n",
        "  ks = np.array(list(dictio.keys()))\n",
        "  vs = np.array(list(dictio.values()))\n",
        "  ks = ks[vs>=threshold]\n",
        "  vs = vs[vs>=threshold]\n",
        "  ord = np.argsort(vs)\n",
        "  ks_n = ks[ord]\n",
        "  return ks_n\n",
        "  \n",
        "\n",
        "def lemmasAndChanges(ws, fs, rs):\n",
        "  \"\"\"\n",
        "    From (lemma, form, M.A.) return (lemma, changes, M.A.)\n",
        "  \"\"\"\n",
        "  cs = list(map(lambda f, w: stringDif(f, w), zip(fs, ws)))\n",
        "  return ws, cs, rs\n",
        "\n",
        "def distanceFromLemma(lemma1, lemma2):\n",
        "  \"\"\"\n",
        "    An example to defining the distance between two lemmas\n",
        "  \"\"\"\n",
        "  d = 0\n",
        "  # Distance is the sum of the character-wise differences\n",
        "  for i in range(min(len(lemma1), len(lemma2))):\n",
        "    d += abs(ord(lemma1[i]) - ord(lemma2[i]))\n",
        "  lemma = lemma1 if len(lemma1) > len(lemma2) else lemma2\n",
        "  # Adding the difference\n",
        "  reste = sum(list(map(lambda a : ord(a),lemma[i+1:])))\n",
        "  return d + reste\n",
        "\n",
        "\n",
        "def distanceFromLemma_(lemma1, lemma2):\n",
        "  \"\"\"\n",
        "    Another example where first and last character are more put to interest than the reste. And no character-wise distance.\n",
        "  \"\"\"\n",
        "  b = 0.5 * (lemma1 == lemma2) + 0.25 * ((lemma1[:2] == lemma2[:2]) + (lemma1[-2:] == lemma2[-2:])) if min(len(lemma1), len(lemma2)) >= 2 else int(lemma1 == lemma2)\n",
        "  return b"
      ],
      "metadata": {
        "id": "fag2cRslEa-c"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def printAccuracies(fdTrain, fdTest):\n",
        "  # Training data and dependencies\n",
        "  (ws, fs, rs), (corresp, corresp2, corresp3) = createTrainDatasetFromFD(fdTrain)\n",
        "  print(\"training=\", len(ws))\n",
        "  probs = fromCorrespToProbs(corresp)\n",
        "  # Test data\n",
        "  (ws_t, fs_t, rs_t), _ = createTrainDatasetFromFD(fdTest)\n",
        "  print(\"Test=\", len(ws_t))\n",
        "  acc, acc2, acc3, acc4 = list(), list(), list(), list()\n",
        "  # Defining a distance that is a sum of the above mentionned ones\n",
        "  dist = lambda a, b : distanceFromLemma(a, b) - distanceFromLemma_(a, b)\n",
        "  # Compare the very naïve approach (Look for elements in data that correspond the most) to a more defined one\n",
        "  for i in range(len(rs_t)):\n",
        "    c = select_less_obvious(ws_t[i], rs_t[i], corresp, corresp2, corresp3, dist)\n",
        "    c2 = selectMaxFromDict(look_obvious_changes(rs_t[i], probs))\n",
        "    # Without taking the lemma to account\n",
        "    acc3.append(stringDif(fs_t[i], ws_t[i]) == c)\n",
        "    acc4.append(stringDif(fs_t[i], ws_t[i]) == c2)\n",
        "    c = c.replace(\"-\", ws_t[i])\n",
        "    c2 = c2.replace(\"-\", ws_t[i])\n",
        "    # Penalize the cases where the lemma is changeable\n",
        "    acc.append(fs_t[i] == c)\n",
        "    acc2.append(fs_t[i] == c2)\n",
        "    \n",
        "  print(\"full-word accuracies are: \\n * Using no additional info:\", np.mean(acc), \"\\n* Using additional info:\", np.mean(acc2), \"\\n Prefix-Suffix accuracies are: \\n * Using no additional info:\", np.mean(acc3), \"\\n * Using additional info\", np.mean(acc4))"
      ],
      "metadata": {
        "id": "8rLAL4ROAl51"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fdTrain = open(\"swa.trn\")\n",
        "fdTest = open(\"swa.tst\")\n",
        "print(\"For Swahli\")\n",
        "printAccuracies(fdTrain, fdTest)\n",
        "fdTrain = open(\"hil.trn\")\n",
        "fdTest = open(\"hil.tst\")\n",
        "print(\"For Hil\")\n",
        "printAccuracies(fdTrain, fdTest)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PYu31cADLSL2",
        "outputId": "959ef927-b5d9-4eec-8b96-81ca39946c03"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For Swahli\n",
            "training= 3374\n",
            "Test= 910\n",
            "full-word accuracies are: \n",
            " * Using no additional info: 0.9263736263736264 \n",
            "* Using additional info: 1.0 \n",
            " Prefix-Suffix accuracies are: \n",
            " * Using no additional info: 0.9263736263736264 \n",
            " * Using additional info 1.0\n",
            "For Hil\n",
            "training= 859\n",
            "Test= 238\n",
            "full-word accuracies are: \n",
            " * Using no additional info: 0.08823529411764706 \n",
            "* Using additional info: 0.46638655462184875 \n",
            " Prefix-Suffix accuracies are: \n",
            " * Using no additional info: 0.09663865546218488 \n",
            " * Using additional info 0.4831932773109244\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fdTrain = open(\"mlg.trn\")\n",
        "fdTest = open(\"mlg.tst\")\n",
        "print(\"For MLG\")\n",
        "printAccuracies(fdTrain, fdTest)\n",
        "\n",
        "fdTrain = open(\"lug.trn\")\n",
        "fdTest = open(\"lug.tst\")\n",
        "print(\"For Lug\")\n",
        "printAccuracies(fdTrain, fdTest)"
      ],
      "metadata": {
        "id": "NNxI1gITaGTR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "351fc27a-3b9e-4b4f-b1b8-03da011ebe79"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For MLG\n",
            "training= 447\n",
            "Test= 127\n",
            "full-word accuracies are: \n",
            " * Using no additional info: 0.9763779527559056 \n",
            "* Using additional info: 1.0 \n",
            " Prefix-Suffix accuracies are: \n",
            " * Using no additional info: 0.9763779527559056 \n",
            " * Using additional info 1.0\n",
            "For Lug\n",
            "training= 3420\n",
            "Test= 977\n",
            "full-word accuracies are: \n",
            " * Using no additional info: 0.3940634595701126 \n",
            "* Using additional info: 0.49437052200614123 \n",
            " Prefix-Suffix accuracies are: \n",
            " * Using no additional info: 0.3930399181166837 \n",
            " * Using additional info 0.5066530194472876\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fdTrain = open(\"krl.trn\")\n",
        "fdTest = open(\"krl.tst\")\n",
        "print(\"For KRL\")\n",
        "printAccuracies(fdTrain, fdTest)\n",
        "\n",
        "fdTrain = open(\"isl.trn\")\n",
        "fdTest = open(\"isl.tst\")\n",
        "print(\"For Isl\")\n",
        "printAccuracies(fdTrain, fdTest)"
      ],
      "metadata": {
        "id": "IudTqI88X0_E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Sa6yrjXHsURf"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "name": "probabilistic_approach.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}