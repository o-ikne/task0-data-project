{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "F3lWRhzSTFO9"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import difflib                        # A library that computes the difference between two strings, unused (was for further exploration and info. PS: related to ngrams)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### General functions"
      ],
      "metadata": {
        "id": "oLxRCACpXanE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "SLU9fXZsTFPU"
      },
      "outputs": [],
      "source": [
        "def suffix_prefix_detection(s, c):\n",
        "  \"\"\"\n",
        "  Looks up the prefix and suffix of a lemma c. The maximal substring in s that correspond to lemma is extracted\n",
        "  :params: s for the form and c for the lemma\n",
        "  :return: the prefix+\"-\"+suffix, where \"-\" replaces the inducted substring of the lemma c\n",
        "  \"\"\"\n",
        "  # Perte d'information, quoi faire pour rattaraper ?\n",
        "  for i in range(len(c),0, -1):\n",
        "    np = c[:i]\n",
        "    ns = s.replace(np, '-')\n",
        "    if ns != s:\n",
        "      # The substring found, and is replaced\n",
        "      break\n",
        "  return ns\n",
        "\n",
        "def change_learned_accuracies_into_probabilities(corresp):\n",
        "  \"\"\"\n",
        "    A function that changes occurences in corresp to probabilities\n",
        "    :Example:\n",
        "    > corresp = {\"pre-é\": {\"V\":7, \"Noun\":3}, \"dé-é\" : {\"V\":8, \"Adj\":2}}\n",
        "    > probs   = {\"pre-é\": {\"V\":0.7, \"Noun\":0.3}, \"dé-é\" : {\"V\":0.8, \"Adj\":0.2}}\n",
        "  \"\"\"\n",
        "  probs = corresp.copy()\n",
        "  for key1 in probs.keys():\n",
        "    sumValues = sum(probs[key1].values())\n",
        "    for key2 in probs[key1].keys():\n",
        "      probs[key1][key2] = probs[key1][key2] / sumValues\n",
        "  return probs\n",
        "\n",
        "def selectMaxFromDict(dictio):\n",
        "  \"\"\"\n",
        "    Selects the key that correspond to the highest value in a dictionary\n",
        "  \"\"\"\n",
        "  ks = np.array(list(dictio.keys()))\n",
        "  vs = np.array(list(dictio.values()))\n",
        "  ord = np.argsort(vs)\n",
        "  ks_n = ks[ord]\n",
        "  return ks_n[0] if not ks_n.shape[0] == 0 else '-'\n",
        "\n",
        "def selectFromDict(dictio, threshold):\n",
        "  \"\"\"\n",
        "    Selects cases (keys) from dictionary where the values are >= threshold. Those are returned ordered from highest to lowest.\n",
        "  \"\"\"\n",
        "  ks = np.array(list(dictio.keys()))\n",
        "  vs = np.array(list(dictio.values()))\n",
        "  ks = ks[vs>=threshold]\n",
        "  vs = vs[vs>=threshold]\n",
        "  ord = np.argsort(vs)\n",
        "  ks_n = ks[ord]\n",
        "  return ks_n\n",
        "\n",
        "def CharByCharAccuracy(lemma1, lemma2):\n",
        "  \"\"\"\n",
        "    An example to defining the distance between two lemmas\n",
        "  \"\"\"\n",
        "  d = suffix_prefix_detection(lemma1, lemma2)\n",
        "  d = d.replace('-', '')\n",
        "  return np.mean(list(map(lambda a:1,d))+[0])/max(len(lemma1), len(lemma2))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Functions exclusively for the naïve version"
      ],
      "metadata": {
        "id": "gCWtCPzUXdqY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def best_candidates_from_probabilities_naive(morphs, probs):\n",
        "  \"\"\"\n",
        "  An approach that searches for potential candidates for the changes.\n",
        "  It simply regroupes the different changes that have been recorded to have had *the exact same* morphological attributes\n",
        "  :params:  * morphs: (String) morphological attributes, mainly on the test data set\n",
        "            * Probs: A dictionary that regroupe changes to their morphological attributes and there occurences on training dataset\n",
        "  :returns: A dictionnary with potential changes to make to a lemma, and the corresponding occurence within the training dataset\n",
        "  \"\"\"\n",
        "  possible_predictions = dict()\n",
        "  for c in probs.keys():\n",
        "    dict_probs = probs[c]\n",
        "    if set(morphs.split(\";\")) == set(dict_probs.keys()):\n",
        "      if c in possible_predictions.keys():\n",
        "        possible_predictions[c] += 1\n",
        "      else:\n",
        "        possible_predictions[c] = 1\n",
        "    # If an example is not met on the training set, we abandon the cause. Hence, obvious\n",
        "  return possible_predictions\n",
        "\n",
        "\n",
        "def train_naive(fd):\n",
        "  \"\"\"\n",
        "    Simple function creating the tuple (lemma, form, M.A) also a dictionnary corresp.\n",
        "    :params: fd is a file descriptor\n",
        "    :return corresp: (dict) The changes \"prefix-suffix\", the attributes and occurences that correspond to them.\n",
        "  \"\"\"\n",
        "  # Create words\n",
        "  l = fd.readline()\n",
        "  ws, fs, rs = list(), list(), list()\n",
        "  while l:\n",
        "      w, f, r = list( map(lambda a : a.strip(), l.split('\\t')) )\n",
        "      ws.append(w); fs.append(f); rs.append(r)\n",
        "      l = fd.readline()\n",
        "  # Create a dictionnary with rule (pre-suf) as keys and have the corresponding morph attributes \n",
        "  corresp = dict()\n",
        "  for i in range(len(ws)):\n",
        "    rule = suffix_prefix_detection(fs[i], ws[i])\n",
        "    if rule in corresp.keys():\n",
        "      for k in rs[i].split(\";\"):\n",
        "        if k in corresp[rule]:\n",
        "          corresp[rule][k] += 1\n",
        "        else:\n",
        "          corresp[rule][k] = 1\n",
        "    else:\n",
        "        corresp[rule] =  {key: 1 for key in rs[i].split(\";\")}\n",
        "  return (ws, fs, rs), corresp\n",
        "\n",
        "def printAccuracies_naive(fdTrain, fdTest):\n",
        "  \"\"\"\n",
        "\tPrints accuracies in the case where the test set disposes actually of forms that are to be predicted\n",
        "  \"\"\"\n",
        "  # Training data and dependencies\n",
        "  (ws, fs, rs), corresp = train_naive(fdTrain)\n",
        "  print(\"Number of the training examples are \", len(ws))\n",
        "  probs = change_learned_accuracies_into_probabilities(corresp)\n",
        "  # Test data\n",
        "  (ws_t, fs_t, rs_t), _ = train_naive(fdTest)\n",
        "  print(\"Number of the testing examples are =\", len(ws_t))\n",
        "  acc, acc2 = list(), list()\n",
        "  # For each test sample\n",
        "  for i in range(len(rs_t)):\n",
        "    c = selectMaxFromDict(best_candidates_from_probabilities_naive(rs_t[i], probs))\n",
        "    # Without taking the lemma to account\n",
        "    c = c.replace(\"-\", ws_t[i])\n",
        "    cbc = CharByCharAccuracy(c, fs_t[i])\n",
        "    acc.append(cbc)\n",
        "    acc2.append(fs_t[i] == c)\n",
        "  print(\"Char-by-char accuracy is:\", 1 - np.mean(acc), \". Word-to-word on the other hand is:\", np.mean(acc2))"
      ],
      "metadata": {
        "id": "etJI5CrspyuV"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Advanced approach methods"
      ],
      "metadata": {
        "id": "QWz0-zZrY54p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def best_candidates_from_probabilities(morphs, probs):\n",
        "  \"\"\"Same as before, but records the cases where only one of the morphological attributes is present\"\"\"\n",
        "  possible_predictions = dict()\n",
        "  for c in probs.keys():\n",
        "    dict_probs = probs[c]\n",
        "    if np.all(list(map(lambda a : a in dict_probs.keys(), morphs.split(\";\")))):\n",
        "      # This is the case where the training set has a sample that had the exacte Morph. Att. and is amplified (+2)\n",
        "      if c in possible_predictions.keys():\n",
        "        possible_predictions[c] += 2\n",
        "      else:\n",
        "        possible_predictions[c] = 2\n",
        "    # The case where at least one morph. att. is present. This case is less representable (+1) \n",
        "    elif np.any(list(map(lambda a : a in dict_probs.keys(), morphs.split(\";\")))):\n",
        "      if c in possible_predictions.keys():\n",
        "        possible_predictions[c] += 1\n",
        "      else:\n",
        "        possible_predictions[c] = 1\n",
        "    # +0 for the case of no presence. This occurences and amplification serve for the probabilistic model later on.\n",
        "  return possible_predictions\n",
        "\n",
        "def best_candidates_from_probabilities_advanced(lemma, morphs, corresp, corresp2, corresp3, dist):\n",
        "  \"\"\"\n",
        "    This is an advanced method. It takes to account some learned information from the training set to \n",
        "    select a potential candidates. It, all the same, is still a naïve approach.\n",
        "    :params:  * corresp: Contains, as probs before, the changes \"prefix-suffix\", the attributes and occurences that correspond to them.\n",
        "              * corresp2: Contains changes and the corresponding lemmas.\n",
        "              * corresp3: Contains the changes and their occurences within the training data set (the world)\n",
        "              * dist: Distance to defines as one pleases between two lemmas\n",
        "    :return: The most probable changes taking to account:\n",
        "    * Their M.A. and those of the test set\n",
        "    * The probability that these changes occur (if they accur rarely in the training set, so they should in the test set)\n",
        "    * The distance of the new lemma from the lemmas in the training set (if this lemma is so close to another one, so the changes are \n",
        "    most probably the same as those on the training set)\n",
        "  \"\"\"\n",
        "  pp = best_candidates_from_probabilities(morphs, change_learned_accuracies_into_probabilities(corresp))\n",
        "  new_preds = dict.fromkeys(pp.keys())\n",
        "  for k in pp.keys():\n",
        "    dists = list(map(lambda a: dist(a, lemma),corresp3[k]))\n",
        "    new_preds[k] = pp[k] * corresp2[k] * (np.max(dists) - np.mean(dists))\n",
        "  minIdx = np.argmax(list(new_preds.values()))\n",
        "  return np.array(list(new_preds.keys()))[minIdx]\n",
        "\n",
        "def train(fd):\n",
        "  \"\"\"\n",
        "    Simple function creating the tuple (lemma, form, M.A) also the tuple (corresp, corresp2, corresp3) as:\n",
        "              * corresp: The changes \"prefix-suffix\", the attributes and occurences that correspond to them.\n",
        "              * corresp2: Contains changes and the corresponding lemmas.\n",
        "              * corresp3: Contains the changes and their occurences within the training data set (the world)\n",
        "    :params: fd is a file descriptor\n",
        "  \"\"\"\n",
        "  # Create words\n",
        "  l = fd.readline()\n",
        "  ws, fs, rs = list(), list(), list()\n",
        "  while l:\n",
        "      w, f, r = list( map(lambda a : a.strip(), l.split('\\t')) )\n",
        "      ws.append(w); fs.append(f); rs.append(r)\n",
        "      l = fd.readline()\n",
        "  # Create a dictionnary with rule (pre-suf) as keys and have the corresponding morph attributes \n",
        "  corresp = dict()\n",
        "  corresp2 = dict()\n",
        "  corresp3 = dict()\n",
        "  for i in range(len(ws)):\n",
        "    rule = suffix_prefix_detection(fs[i], ws[i])\n",
        "    if rule in corresp.keys():\n",
        "      corresp2[rule] += 1\n",
        "      corresp3[rule].append(ws[i])\n",
        "      for k in rs[i].split(\";\"):\n",
        "        if k in corresp[rule]:\n",
        "          corresp[rule][k] += 1\n",
        "        else:\n",
        "          corresp[rule][k] = 1\n",
        "    else:\n",
        "        corresp[rule] =  {key: 1 for key in rs[i].split(\";\")}\n",
        "        corresp2[rule] = 1\n",
        "        corresp3[rule] = [ws[i]]\n",
        "  return (ws, fs, rs), (corresp, corresp2, corresp3)"
      ],
      "metadata": {
        "id": "4UUdQ57cY9f8"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def distanceFromLemma(lemma1, lemma2):\n",
        "  \"\"\"\n",
        "    An example to defining the distance between two lemmas\n",
        "  \"\"\"\n",
        "  d = 0\n",
        "  # Distance is the sum of the character-wise differences\n",
        "  for i in range(min(len(lemma1), len(lemma2))):\n",
        "    d += abs(ord(lemma1[i]) - ord(lemma2[i]))\n",
        "  lemma = lemma1 if len(lemma1) > len(lemma2) else lemma2\n",
        "  # Adding the difference\n",
        "  reste = sum(list(map(lambda a : ord(a),lemma[i+1:])))\n",
        "  return d + reste\n",
        "\n",
        "def distanceFromLemma_(lemma1, lemma2):\n",
        "  \"\"\"\n",
        "    Another example where first and last character are more put to interest than the reste. And no character-wise distance.\n",
        "  \"\"\"\n",
        "  b = 0.5 * (lemma1 == lemma2) + 0.25 * ((lemma1[:2] == lemma2[:2]) + (lemma1[-2:] == lemma2[-2:])) if min(len(lemma1), len(lemma2)) >= 2 else int(lemma1 == lemma2)\n",
        "  return b"
      ],
      "metadata": {
        "id": "fag2cRslEa-c"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def printAccuracies(fdTrain, fdTest):\n",
        "  \"\"\"\n",
        "\tPrints accuracies in the case where the test set disposes actually of forms that are to be predicted\n",
        "  \"\"\"\n",
        "  # Training data and dependencies\n",
        "  (ws, fs, rs), (corresp, corresp2, corresp3) = train(fdTrain)\n",
        "  print(\"Number of the training examples are \", len(ws))\n",
        "  probs = change_learned_accuracies_into_probabilities(corresp)\n",
        "  # Test data\n",
        "  (ws_t, fs_t, rs_t), _ = train(fdTest)\n",
        "  print(\"Number of the testing examples are =\", len(ws_t))\n",
        "  acc, acc2 = list(), list()\n",
        "  # Defining a distance that is a sum of the above mentionned ones\n",
        "  dist = lambda a, b : distanceFromLemma(a, b) - distanceFromLemma_(a, b)\n",
        "  # For each test sample\n",
        "  for i in range(len(rs_t)):\n",
        "    c = best_candidates_from_probabilities_advanced(ws_t[i], rs_t[i], corresp, corresp2, corresp3, dist)\n",
        "    # Without taking the lemma to account\n",
        "    c = c.replace(\"-\", ws_t[i])\n",
        "    cbc = CharByCharAccuracy(c, fs_t[i])\n",
        "    acc.append(cbc)\n",
        "    acc2.append(fs_t[i] == c)\n",
        "    \n",
        "  print(\"Char-by-char accuracy is:\", 1 - np.mean(acc), \". Word-to-word on the other hand is:\", np.mean(acc2))"
      ],
      "metadata": {
        "id": "8rLAL4ROAl51"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Demonstrations"
      ],
      "metadata": {
        "id": "RVP2vS3vYwYT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fdTrain = open(\"swa.trn\")\n",
        "fdTest = open(\"swa.tst\")\n",
        "print(\"For Swahli naïve\")\n",
        "printAccuracies_naive(fdTrain, fdTest)\n",
        "print(\"For Swahli advanced\")\n",
        "fdTrain = open(\"swa.trn\")\n",
        "fdTest = open(\"swa.tst\")\n",
        "printAccuracies(fdTrain, fdTest)\n",
        "fdTrain = open(\"hil.trn\")\n",
        "fdTest = open(\"hil.tst\")\n",
        "print(\"For Hil naïve\")\n",
        "printAccuracies_naive(fdTrain, fdTest)\n",
        "fdTrain = open(\"hil.trn\")\n",
        "fdTest = open(\"hil.tst\")\n",
        "print(\"For Hil advanced\")\n",
        "printAccuracies(fdTrain, fdTest)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PYu31cADLSL2",
        "outputId": "cefb4abb-bc66-4690-f296-21993d0afb25"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For Swahli naïve\n",
            "Number of the training examples are  3374\n",
            "Number of the testing examples are = 910\n",
            "Char-by-char accuracy is: 1.0 . Word-to-word on the other hand is: 1.0\n",
            "For Swahli advanced\n",
            "Number of the training examples are  3374\n",
            "Number of the testing examples are = 910\n",
            "Char-by-char accuracy is: 0.9927754772699827 . Word-to-word on the other hand is: 0.9263736263736264\n",
            "For Hil naïve\n",
            "Number of the training examples are  859\n",
            "Number of the testing examples are = 238\n",
            "Char-by-char accuracy is: 0.9594787565649753 . Word-to-word on the other hand is: 0.46638655462184875\n",
            "For Hil advanced\n",
            "Number of the training examples are  859\n",
            "Number of the testing examples are = 238\n",
            "Char-by-char accuracy is: 0.9290535123003405 . Word-to-word on the other hand is: 0.08823529411764706\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fdTrain = open(\"mlg.trn\")\n",
        "fdTest = open(\"mlg.tst\")\n",
        "print(\"For MLG naïve\")\n",
        "printAccuracies_naive(fdTrain, fdTest)\n",
        "print(\"For MLG advanced\")\n",
        "fdTrain = open(\"mlg.trn\")\n",
        "fdTest = open(\"mlg.tst\")\n",
        "printAccuracies(fdTrain, fdTest)\n",
        "\n",
        "fdTrain = open(\"lug.trn\")\n",
        "fdTest = open(\"lug.tst\")\n",
        "print(\"For LUG naïve\")\n",
        "printAccuracies_naive(fdTrain, fdTest)\n",
        "print(\"For LUG advanced\")\n",
        "fdTrain = open(\"lug.trn\")\n",
        "fdTest = open(\"lug.tst\")\n",
        "printAccuracies(fdTrain, fdTest)"
      ],
      "metadata": {
        "id": "NNxI1gITaGTR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48cce901-b394-4028-f145-e1d4fe4adb77"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For MLG naïve\n",
            "Number of the training examples are  447\n",
            "Number of the testing examples are = 127\n",
            "Char-by-char accuracy is: 1.0 . Word-to-word on the other hand is: 1.0\n",
            "For MLG advanced\n",
            "Number of the training examples are  447\n",
            "Number of the testing examples are = 127\n",
            "Char-by-char accuracy is: 0.9979241231209736 . Word-to-word on the other hand is: 0.9763779527559056\n",
            "For LUG naïve\n",
            "Number of the training examples are  3420\n",
            "Number of the testing examples are = 977\n",
            "Char-by-char accuracy is: 0.9596160346407826 . Word-to-word on the other hand is: 0.49437052200614123\n",
            "For LUG advanced\n",
            "Number of the training examples are  3420\n",
            "Number of the testing examples are = 977\n",
            "Char-by-char accuracy is: 0.9450548821756142 . Word-to-word on the other hand is: 0.3940634595701126\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fdTrain = open(\"isl.trn\")\n",
        "fdTest = open(\"isl.tst\")\n",
        "print(\"For ISL naïve\")\n",
        "printAccuracies_naive(fdTrain, fdTest)\n",
        "print(\"For ISL advanced\")\n",
        "fdTrain = open(\"isl.trn\")\n",
        "fdTest = open(\"isl.tst\")\n",
        "printAccuracies(fdTrain, fdTest)\n",
        "\n",
        "fdTrain = open(\"krl.trn\")\n",
        "fdTest = open(\"krl.tst\")\n",
        "print(\"For KRL naïve\")\n",
        "printAccuracies_naive(fdTrain, fdTest)\n",
        "print(\"For KRL advanced\")\n",
        "fdTrain = open(\"krl.trn\")\n",
        "fdTest = open(\"krl.tst\")\n",
        "printAccuracies(fdTrain, fdTest)"
      ],
      "metadata": {
        "id": "IudTqI88X0_E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f126feef-2e80-4651-a405-9a2101b4dbe0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For ISL naïve\n",
            "Number of the training examples are  53841\n",
            "Number of the testing examples are = 15384\n",
            "Char-by-char accuracy is: 0.9276864037013997 . Word-to-word on the other hand is: 0.06415756630265211\n",
            "For ISL advanced\n",
            "Number of the training examples are  53841\n",
            "Number of the testing examples are = 15384\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Unused functions"
      ],
      "metadata": {
        "id": "U6HHnNdKWtAI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "cCxi6d94TFPR"
      },
      "outputs": [],
      "source": [
        "def look_changes(morphs, probs, threshold):\n",
        "  \"\"\"\n",
        "  A very naïve approach that searches for potential candidates for the changes.\n",
        "  It simply regroupes the different changes that have been recorded to have had one amongst morphological attributes\n",
        "  :params:  * morphs: (array) morphological attributes, mainly on the test data set\n",
        "            * Probs: A dictionary that regroupe changes to their morphological attributes and there occurences on training dataset\n",
        "            * Threshold: Unused, but serves as an indication to ignore underrated examples\n",
        "  :returns: A dictionnary with potential changes to make to a lemma, and the corresponding accumulative probabilities\n",
        "  within the training dataset\n",
        "  \"\"\"\n",
        "  possible_predictions = dict()\n",
        "  for c in probs.keys():\n",
        "    dict_probs = probs[c]\n",
        "    for morph in morphs:\n",
        "      if morph in dict_probs.keys() and dict_probs[morph] >= threshold:\n",
        "        if c in possible_predictions.keys():\n",
        "          possible_predictions[c] += dict_probs[morph]\n",
        "        else:\n",
        "          possible_predictions[c] = dict_probs[morph]\n",
        "  return possible_predictions\n",
        "\n",
        "def generate_test(fd):\n",
        "  \"\"\"\n",
        "    In the case of test datasets, the tuple (lemma, M.A) only is returned. Not used as we are in demo mode ;)\n",
        "  \"\"\"\n",
        "  i = 0\n",
        "  l = fd.readline()\n",
        "  ws, rs = list(), list()\n",
        "  while l:\n",
        "      w, r = list( map(lambda a : a.strip(), l.split('\\t')) )\n",
        "      ws.append(w); rs.append(r)\n",
        "      l = fd.readline()\n",
        "      i += 1\n",
        "      #if i > 700:\n",
        "        #break\n",
        "  return (ws, rs)\n",
        "\n",
        "def lemmasAndChanges(ws, fs, rs):\n",
        "  \"\"\"\n",
        "    From (lemma, form, M.A.) return (lemma, changes, M.A.)\n",
        "  \"\"\"\n",
        "  cs = list(map(lambda f, w: suffix_prefix_detection(f, w), zip(fs, ws)))\n",
        "  return ws, cs, rs"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "name": "probabilistic_approach.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}